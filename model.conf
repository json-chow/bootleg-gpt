[tokenizer]
vocab_file = vocab.json
merges_file = merges.txt

[training]
# Where to output the trained model
out_dir = out
# Name to give the trained model
out_name = bootleg_gpt
# Location of training/validation data
data_loc = data/wikitext-103-raw
# Number of batches of data at a time to feed to the model
batch_size = 8
# Number of training iterations
max_iters = 100000
# Number of evaluation iterations
eval_iters = 100
# AdamW learning rate
lr = 1e-4
# AdamW weight decay
weight_decay = 1e-2

[model]
# Max context length
n_ctx = 512
# Embedding size
n_embd = 256
# Number of transformer blocks
n_layer = 6
# Number of attention heads
n_head = 8
# Dimensionality of feedforward layers, default is 4 * n_embd
n_inner = 1024
# Dropout probability
drop = 0
# Epsilon for layer norm layers
layer_norm_eps = 1e-5
# Include bias in linear layers
bias = False
# Which device to run on
device = None

[output]
out_txt = out.txt
