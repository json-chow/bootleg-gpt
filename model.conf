[tokenizer]
vocab_file = vocab.json
merges_file = merges.txt
# Number of processes for tokenizing
n_jobs = 1

[training]
# Where to output the trained model
out_dir = out
# Location of training/validation data
data_loc = data/wikitext-103-raw
# Number of batches of data to feed to the model
batch_size = 6
# Number of training iterations
max_iters = 10000

[model]
# Max context length
n_ctx = 1024
# Embedding size
n_embd = 768
# Number of transformer blocks
n_layer = 12
# Number of attention heads
n_head = 12
# Dimensionality of feedforward layers, default is 4 * n_embd
n_inner = None
# Dropout probability
drop = 0.1
# Epsilon for layer norm layers
layer_norm_eps = 1e-5
# Include bias in linear layers
bias = True
# Which device to run on
device = None

